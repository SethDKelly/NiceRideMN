{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Dependencies and library requirements_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# standard libraries\n",
    "import numpy as np #numpy warnings, if any, may be safely ignored, known issue.\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from math import * # Used for the haversine/distance calculations\n",
    "\n",
    "# for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for model building\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# for optimizations\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Loading in station data by year. Hierarchal dataframe with levels based on year_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ridership_dict = defaultdict()\n",
    "station_dict = defaultdict()\n",
    "\n",
    "for x in [2010 + x for x in range(8)] :\n",
    "    station_dict[x] = pd.read_csv(\"https://raw.githubusercontent.com/SethDKelly/NiceRideMN/master/Nice_Ride_data/\" \\\n",
    "                             +str(x)+\"/NiceRide_station_\"+str(x)+\".csv\")\n",
    "    ridership_dict[x] = pd.read_csv(\"https://raw.githubusercontent.com/SethDKelly/NiceRideMN/master/Nice_Ride_data/\" \\\n",
    "                             +str(x)+\"/NiceRide_trip_history_\"+str(x)+\".csv\")\n",
    "NR_ridership = pd.concat(ridership_dict)    \n",
    "NR_station = pd.concat(station_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Due to station distance matrices being different sizes current load pattern is by manual selection of year._"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def kmeans_builder(NR_station, start=2010, end=8) :\n",
    "    \n",
    "    # For KMeans model building\n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    # For Feature Optimizations\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    #########################################################################\n",
    "    # This sections builds the clusters for each (2010-2017) years data\n",
    "    # The code uses KMeans clustering with a K value of 6\n",
    "    # Appends to the DF `NR_station` the cluster each station belongs\n",
    "    # clustering data is inserted into a new column `Cluster`\n",
    "    ########################################################################\n",
    "\n",
    "    NR_station['Cluster'] = int(0)\n",
    "\n",
    "    k_val = 6\n",
    "    \n",
    "    for year in [start + x for x in range(end)] :\n",
    "    \n",
    "        distance_matrix = pd.DataFrame() # Reinstantiate the  Distance Matrix DataFrame for clean run\n",
    "    \n",
    "        # Read in the distance matrix for the particular year\n",
    "        distance_matrix = pd.read_csv(\"https://raw.githubusercontent.com/SethDKelly/NiceRideMN/master/Nice_Ride_data/\" \\\n",
    "                               +str(year)+\"/distance_matrix_\"+str(year)+\".csv\",index_col=0)\n",
    "        \n",
    "        # Optimize distance matrix to two primary x-y components\n",
    "        pca = PCA(n_components=2).fit_transform(distance_matrix)\n",
    "        \n",
    "        # assert len(NR_station.loc[year, 'Cluster'].values) == \n",
    "        kmeans = KMeans(n_clusters=k_val, n_init=200).fit(pca)\n",
    "        centers = kmeans.cluster_centers_\n",
    "\n",
    "        for x in range(19) :\n",
    "    \n",
    "            # Reduce the dimensionality of our distance matrix\n",
    "            kmeans = KMeans(n_clusters=k_val, n_init=100).fit(pca)\n",
    "            centers += kmeans.cluster_centers_\n",
    "            \n",
    "            centered = centers / 20\n",
    "            #Append to the DF `NR_station` the cluster each station belongs to in column `Cluster`\n",
    "            NR_station.loc[year, 'Cluster'] =  KMeans(n_clusters=k_val, init=centered, n_init=10).fit(pca).labels_\n",
    "        \n",
    "    return NR_station"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nr_station = kmeans_builder(NR_station)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for year in [2010 + x for x in range(8)] :\n",
    "\n",
    "    plt.figure(figsize=[15,15])\n",
    "    plt.xlim([-93.35,-93.05])\n",
    "    plt.ylim([44.88,45.05])\n",
    "    \n",
    "    for k in range(0,6):\n",
    "        temp = nr_station.loc[year, :][nr_station.loc[year, 'Cluster'] == k]\n",
    "        ax = plt.scatter(temp.Longitude, temp.Latitude, label='Cluster'+str(k))\n",
    "    \n",
    "        ax = plt.gca()\n",
    "        ax.set_title(str(year)+' Clusters')\n",
    "        ax.legend()\n",
    "    plt.savefig(\"/home/grimoire/Projects/NiceRide/Visualizations/Clusters/\"+str(year)+\"_clustering.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2017\n",
    "\n",
    "start = NR_ridership.loc[year,:]['Start_id'].values\n",
    "end = NR_ridership.loc[year,:]['End_id'].values\n",
    "\n",
    "# Build a data frame with start terminal as indices, end terminal as columns filled with zeros\n",
    "station_ids = nr_station.loc[year,:].Terminal.values\n",
    "travel_df = pd.DataFrame(columns = station_ids, index = station_ids).fillna(0)\n",
    "\n",
    "for x in range(len(start)) :\n",
    "    travel_df.loc[start[x], end[x]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "clus_array = nr_station.Cluster.values\n",
    "term_cluster_pair = {}\n",
    "for x in range(len(station_ids)) :\n",
    "    term_cluster_pair[station_ids[x]] = clus_array[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Build this better using lambda mapping and dictionaries\n",
    "'''\n",
    "travel_to = pd.DataFrame(columns=['to_0', 'to_1', 'to_2', 'to_3', 'to_4', 'to_5'], \\\n",
    "                              index=station_ids).fillna(0)\n",
    "\n",
    "for x in station_ids :\n",
    "    if term_cluster_pair[x] == 0:\n",
    "        travel_to['to_0'] += (travel_df[x].values)\n",
    "    elif term_cluster_pair[x] == 1:\n",
    "        travel_to['to_1'] += (travel_df[x].values)\n",
    "    elif term_cluster_pair[x] == 2:\n",
    "        travel_to['to_2'] += (travel_df[x].values)\n",
    "    elif term_cluster_pair[x] == 3:\n",
    "        travel_to['to_3'] += (travel_df[x].values)\n",
    "    elif term_cluster_pair[x] == 4:\n",
    "        travel_to['to_4'] += (travel_df[x].values)\n",
    "    elif term_cluster_pair[x] == 5:\n",
    "        travel_to['to_5'] += (travel_df[x].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_matrix = travel_df.values\n",
    "from_matrix = travel_matrix.transpose(1,0)\n",
    "from_df = pd.DataFrame(from_matrix, columns=station_ids, index=station_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Build this better using lambda mapping and dictionaries\n",
    "'''\n",
    "\n",
    "travel_from = pd.DataFrame(columns=['from_0', 'from_1', 'from_2', 'from_3', 'from_4', 'from_5'], \\\n",
    "                              index=station_ids).fillna(0)\n",
    "\n",
    "for x in station_ids :\n",
    "    if term_cluster_pair[x] == 0:\n",
    "        travel_from['from_0'] += (from_df[x].values)\n",
    "    elif term_cluster_pair[x] == 1:\n",
    "        travel_from['from_1'] += (from_df[x].values)\n",
    "    elif term_cluster_pair[x] == 2:\n",
    "        travel_from['from_2'] += (from_df[x].values)\n",
    "    elif term_cluster_pair[x] == 3:\n",
    "        travel_from['from_3'] += (from_df[x].values)\n",
    "    elif term_cluster_pair[x] == 4:\n",
    "        travel_from['from_4'] += (from_df[x].values)\n",
    "    elif term_cluster_pair[x] == 5:\n",
    "        travel_from['from_5'] += (from_df[x].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_summarize = pd.merge(travel_from, travel_to, left_index=True, right_index=True, how='outer').fillna(0)\n",
    "to_summarize = pd.merge(to_summarize, nr_station.loc[year,:], left_index=True, right_on='Terminal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_to_summary = pd.DataFrame(columns=['clus0', 'clus1', 'clus2','clus3', 'clus4', 'clus5', 'pct_intra'],\\\n",
    "                    index=['Cluster_'+str(x) for x in np.unique(to_summarize.Cluster.values)]).fillna(0)\n",
    "\n",
    "for x in np.unique(to_summarize.Cluster.values) :\n",
    "    intra_to_summary.loc['Cluster_'+str(x)]['clus0'] = to_summarize[to_summarize.Cluster == x]['to_0'].values.sum()\n",
    "    intra_to_summary.loc['Cluster_'+str(x)]['clus1'] = to_summarize[to_summarize.Cluster == x]['to_1'].values.sum()\n",
    "    intra_to_summary.loc['Cluster_'+str(x)]['clus2'] = to_summarize[to_summarize.Cluster == x]['to_2'].values.sum()\n",
    "    intra_to_summary.loc['Cluster_'+str(x)]['clus3'] = to_summarize[to_summarize.Cluster == x]['to_3'].values.sum()\n",
    "    intra_to_summary.loc['Cluster_'+str(x)]['clus4'] = to_summarize[to_summarize.Cluster == x]['to_4'].values.sum()\n",
    "    intra_to_summary.loc['Cluster_'+str(x)]['clus5'] = to_summarize[to_summarize.Cluster == x]['to_5'].values.sum()\n",
    "    intra_to_summary.loc['Cluster_'+str(x)]['pct_intra'] = (intra_to_summary.loc['Cluster_'+str(x)]['clus'+str(x)]\\\n",
    "                                                            / intra_to_summary.loc['Cluster_'+str(x),:].values.sum())\\\n",
    "                                                            *100\n",
    "        \n",
    "intra_from_summary = pd.DataFrame(columns=['clus0', 'clus1', 'clus2','clus3', 'clus4', 'clus5', 'pct_intra'],\\\n",
    "                    index=['Cluster_'+str(x) for x in np.unique(to_summarize.Cluster.values)]).fillna(0)\n",
    "\n",
    "for x in np.unique(to_summarize.Cluster.values) :\n",
    "    intra_from_summary.loc['Cluster_'+str(x)]['clus0'] = to_summarize[to_summarize.Cluster == x]['from_0'].values.sum()\n",
    "    intra_from_summary.loc['Cluster_'+str(x)]['clus1'] = to_summarize[to_summarize.Cluster == x]['from_1'].values.sum()\n",
    "    intra_from_summary.loc['Cluster_'+str(x)]['clus2'] = to_summarize[to_summarize.Cluster == x]['from_2'].values.sum()\n",
    "    intra_from_summary.loc['Cluster_'+str(x)]['clus3'] = to_summarize[to_summarize.Cluster == x]['from_3'].values.sum()\n",
    "    intra_from_summary.loc['Cluster_'+str(x)]['clus4'] = to_summarize[to_summarize.Cluster == x]['from_4'].values.sum()\n",
    "    intra_from_summary.loc['Cluster_'+str(x)]['clus5'] = to_summarize[to_summarize.Cluster == x]['from_5'].values.sum()\n",
    "    intra_from_summary.loc['Cluster_'+str(x)]['pct_intra'] = (intra_from_summary.loc['Cluster_'+str(x)]['clus'+str(x)]\\\n",
    "                                                              / intra_from_summary.loc['Cluster_'+str(x),:].values.sum())\\\n",
    "                                                              *100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To-From\n",
      "           clus0  clus1  clus2  clus3  clus4  clus5  pct_intra\n",
      "Cluster_0  63163  26517  25154  22719  15176  47181         31\n",
      "Cluster_1  12393     95   2728   5269     35    141          0\n",
      "Cluster_2   4342   1003   4009   1219    560   1123         32\n",
      "Cluster_3  27202   2340   6996  10078   2988   7519         17\n",
      "Cluster_4  30507  58893   4104  14644  24707  18170         16\n",
      "Cluster_5   3329   4757   3699   2661   2215   2267         11 \n",
      "\n",
      "From-To\n",
      "           clus0  clus1  clus2  clus3  clus4  clus5  pct_intra\n",
      "Cluster_0  62361  27216  23759  21372  15767  45761         31\n",
      "Cluster_1  12105     79   3067   5409     32     72          0\n",
      "Cluster_2   4112   1041   4131   1346    502   1154         33\n",
      "Cluster_3  27773   2437   7535  10220   3530   8693         16\n",
      "Cluster_4  30432  58319   3537  15540  24847  17804         16\n",
      "Cluster_5   3487   4797   3744   2833   2301   2788         13\n"
     ]
    }
   ],
   "source": [
    "print('To-From')\n",
    "print(intra_to_summary,'\\n')\n",
    "print('From-To')\n",
    "print(intra_from_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = intra_to_summary.drop(['clus0', 'clus3', 'clus4'], axis=1).rename(\\\n",
    "                                                    {\"clus1\":'stp', 'clus2':'midway','clus5':'cedar'}, axis=1)\n",
    "test2 = intra_from_summary.drop(['clus0', 'clus3', 'clus4'], axis=1).rename(\\\n",
    "                                                    {\"clus1\":'stp', 'clus2':'midway','clus5':'cedar'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = test1.drop(['Cluster_0', 'Cluster_3', 'Cluster_4'], axis=0).rename(\\\n",
    "                                                    {\"Cluster_1\":'stp', 'Cluster_2':'midway','Cluster_5':'cedar'}, axis=0)\n",
    "test2 = test2.drop(['Cluster_0', 'Cluster_3', 'Cluster_4'], axis=0).rename(\\\n",
    "                                                    {\"Cluster_1\":'stp', 'Cluster_2':'midway','Cluster_5':'cedar'}, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To-From\n",
      "         stp  midway  cedar  pct_intra\n",
      "stp       95    2728    141          0\n",
      "midway  1003    4009   1123         32\n",
      "cedar   4757    3699   2267         11 \n",
      "\n",
      "From-To\n",
      "         stp  midway  cedar  pct_intra\n",
      "stp       79    3067     72          0\n",
      "midway  1041    4131   1154         33\n",
      "cedar   4797    3744   2788         13\n"
     ]
    }
   ],
   "source": [
    "print('To-From')\n",
    "print(test1,'\\n')\n",
    "print('From-To')\n",
    "print(test2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
