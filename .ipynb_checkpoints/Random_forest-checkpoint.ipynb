{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## To show this notebook as a slideshow, after downloading this notebook, change to the directory you have store the file in and type the following into the terminal/console:\n",
    "\n",
    "jupyter nbconvert Random_forest.ipynb --to slides --post serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Obligatory dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# importing dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For model building\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This notebook builds a random forest decision tree for prediction daily ride patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "weather = pd.read_csv(\"~/Projects/NiceRide/Weather_data/01012010_12312017.csv\") # Weather data\n",
    "#weather = pd.read_csv('/home/gilmore/NiceRideMN/Weather_data/01012010_12312017.csv') # weather alt\n",
    "\n",
    "temp = []\n",
    "for x in [2013 + x for x in range(5)] :\n",
    "    x = pd.read_csv(\"~/Projects/NiceRide/Nice_Ride_data/\"+str(x)+\"/NiceRide_trip_history_\"+str(x)+\".csv\")\n",
    "    #x = pd.read_csv('/home/gilmore/NiceRideMN/Nice_Ride_data/'+str(x)+'/NiceRide_trip_history_'+str(x)+'.csv')\n",
    "    temp.append(x)\n",
    "    nr = pd.concat(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Changing the datetime objects to datetime variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "weather['DATE'] = weather['DATE'].apply(lambda x: dt.datetime.strptime(x,'%Y-%m-%d'))\n",
    "\n",
    "nr['Start_date'] = nr['Start_date'].apply(lambda x: dt.datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "nr['End_date'] = nr['End_date'].apply(lambda x: dt.datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparing the data to be used for the model\n",
    "* Using datetime variable rides will be grouped by:\n",
    " * Daily count.\n",
    " * Mean ride duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Resampling our NR data so it takes the start date and Total duration, returns a count of rides per day 'daily_totals'\n",
    "\n",
    "daily_totals = nr[['Start_date', 'duration']].resample('D', on='Start_date').count()\n",
    "daily_totals = daily_totals.drop('Start_date', axis=1)\n",
    "daily_totals = daily_totals.reset_index()\n",
    "daily_totals = daily_totals.rename(index=str, columns={\"duration\": \"daily_count\",'Start_date':'DATE'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "daily_means = nr[['Start_date', 'duration']].resample('D', on='Start_date').mean()\n",
    "daily_means = daily_means.reset_index()\n",
    "daily_means = daily_means.rename(index=str, columns={\"duration\": \"daily_mean\",'Start_date':'DATE'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Correcting missing data in the weather dataframe\n",
    "* From 2010-2013 There isn't TAVG this will be ameliorated by using the mean of TMIN and TMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Changing TAVG-NAN values to the mean of the TMAX and TMIN\n",
    "weather = weather.fillna(value={'TAVG': weather[weather.TAVG.isna()][['TMAX', 'TMIN']]\n",
    "                                .agg(\"mean\", axis=\"columns\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Dropping columns that won't be used in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "weather = weather.drop(['STATION', 'NAME'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now though our data is corrected for the analysis it will be merged together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "temp = weather.merge(daily_totals, how='outer',on='DATE')\n",
    "temp = temp.merge(daily_means, how='outer',on='DATE')\n",
    "temp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Notice how the count of DCOUNT varies from what the other columns have\n",
    "features = temp.fillna(value=0,axis=0) # Where we don't have a daily count (Dcount) fill this with zeros instead\n",
    "features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This next section will change our date column from dtype datetime into seperate numerical relevant date colummns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "features['YEAR'] = features['DATE'].dt.year\n",
    "features['MONTH'] = features['DATE'].dt.month\n",
    "features['DAY'] = features['DATE'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "features.head(5) # Three new, seperate, columns that identify year, month, day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature engineering to create variables:\n",
    "* Weekday or weekend\n",
    "* Summer or not summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create a weekday variable column Mon = 0, Sun = 6\n",
    "features['WEND'] = features['DATE'].dt.weekday\n",
    "\n",
    "# Adjust WEND column to be catagorical; If it's a weekday WEND = 0 else if weekend WEND = 1\n",
    "features['WEND'] = features['WEND'].apply(lambda x:1 if x>4 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Random forests can't use datetime objects\n",
    "features = features.drop('DATE', axis=1)\n",
    "features = features.drop(['SNWD', 'SNOW'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Checking our dataset prior to data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Splitting the data on days with Daily Counts and without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Seperating our data sets\n",
    "winter_features = features[features['daily_count'] == 0].reset_index(drop=True)\n",
    "\n",
    "# Need to drop all days where there where no rides taken (Dcount = 0)\n",
    "features = features[features['daily_count'] != 0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DCOUNT shows Minimum of at least 1, and the\n",
    "\n",
    "## I'm concerned why month shows min 1; there shouldn't be any data for January. Time to investigate further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "features[features.MONTH == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Albeit rare, above freezing in January can happen in Minnesota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Features, Targets, and converting Data in arrays for performance optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature (variable) and Target (Daily rides) creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the target/label variable and removing from features Dataframe\n",
    "target = features.daily_count.values\n",
    "feat = features.drop('daily_count', axis=1).values\n",
    "\n",
    "# Saving the feature names for future use\n",
    "feature_names = list(features.drop('daily_count', axis=1).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Splitting our data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the data into training and test sets\n",
    "X, X_test, y, y_test = train_test_split(feat, target, test_size=.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Inspecting the shape of the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print('Training Features shape', X.shape)\n",
    "print('Training labels shape', y.shape)\n",
    "\n",
    "print('Testing Features shape', X_test.shape)\n",
    "print('Testing labels shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators=1000, random_state=42)\n",
    "\n",
    "rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Making predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predictions = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Assessing the performance of our RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "ex_var_score = explained_variance_score(y_test, predictions)\n",
    "m_absolute_error = mean_absolute_error(y_test, predictions)\n",
    "m_squared_error = mean_squared_error(y_test, predictions)\n",
    "r_2_score = r2_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Explained Variance Score: \"+str(ex_var_score))\n",
    "print(\"Mean Absolute Error \"+str(m_absolute_error))\n",
    "print(\"Root Mean Squared Error \"+str(np.sqrt(m_squared_error)))\n",
    "print(\"R Squared Score \"+str(r_2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# These are the 4th degree polynomial benchmarks\n",
    "* Explained Variance Score: 0.5826\n",
    "* Mean Absolute Error: 460.3430\n",
    "* Root Mean Squared Error: 616.5765\n",
    "* R Squared Score: 0.5745"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# These are the Random Forest Regression benchmarks\n",
    "* Explained Variance Score: 0.8073\n",
    "* Mean Absolute Error: 301.7230\n",
    "* Root Mean Squared Error: 393.9897\n",
    "* R Squared Score: 0.8038"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Percent improved from 4th poly to Random Forest Regression:\n",
    " * Explained Variance Score: 38.57%\n",
    " * Mean Absolute Error 34.46%\n",
    " * Root Mean Squared Error 36.10%\n",
    " * R Squared Score 39.91%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Looking at the feature importance to the decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_names, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualizing the feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Set the style\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "plt.figure(figsize=[14,8])\n",
    "\n",
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical')\n",
    "\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, feature_names, rotation='vertical')\n",
    "\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Graphing the emperical data and the predictions\n",
    "* This will be presented as:\n",
    " * Emperical as a line graph\n",
    " * Predictions will be as a 'o' marker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Dates of training values\n",
    "days = feat[:, feature_names.index('DAY')]\n",
    "months = feat[:, feature_names.index('MONTH')]\n",
    "years = feat[:, feature_names.index('YEAR')]\n",
    "\n",
    "# List and then convert to datetime object\n",
    "dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\n",
    "dates = [dt.datetime.strptime(date, '%Y-%m-%d') for date in dates]\n",
    "\n",
    "# Dataframe with true values and dates\n",
    "true_data = pd.DataFrame(data = {'date': dates, 'DCOUNT': target})\n",
    "\n",
    "# Dates of predictions\n",
    "months = X_test[:, feature_names.index('MONTH')]\n",
    "days = X_test[:, feature_names.index('DAY')]\n",
    "years = X_test[:, feature_names.index('YEAR')]\n",
    "\n",
    "# Column of dates\n",
    "test_dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\n",
    "\n",
    "# Convert to datetime objects\n",
    "test_dates = [dt.datetime.strptime(date, '%Y-%m-%d') for date in test_dates]\n",
    "\n",
    "# Dataframe with predictions and dates\n",
    "predictions_data = pd.DataFrame(data = {'date': test_dates, 'prediction': predictions})\n",
    "\n",
    "# Plot the actual values\n",
    "plt.figure(figsize=(14.5,10))\n",
    "plt.plot(true_data['date'], true_data['DCOUNT'], 'b', label = 'Daily Count', linewidth=1, alpha=.6)\n",
    "\n",
    "# Plot the predicted values\n",
    "plt.plot(predictions_data['date'], predictions_data['prediction'], 'ro', label = 'prediction', markersize=3, alpha=.6)\n",
    "plt.xticks(rotation = '60'); \n",
    "plt.legend()\n",
    "\n",
    "# Graph labels\n",
    "plt.xlabel('Date'); plt.ylabel('Daily Rides'); plt.title('Actual and Predicted Values');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scatter plot of: \n",
    "* Emperical Data vs Max temperature (highest feature)\n",
    "* Prediction data vs Max temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Dataframe with true values and dates\n",
    "true_data = pd.DataFrame(data = {'TMAX': features.TMAX.values, 'DCOUNT': target})\n",
    "# Dataframe with predictions and dates\n",
    "predictions_data = pd.DataFrame(data = {'TMAX': X_test[:,3], 'prediction': predictions})\n",
    "# Plot the actual values and predicted values\n",
    "plt.figure(figsize=(14.5,10))\n",
    "plt.plot(true_data['TMAX'], true_data['DCOUNT'], 'bo', label = 'Daily Count', markersize=3, alpha=.6)\n",
    "plt.plot(predictions_data['TMAX'], predictions_data['prediction'], 'ro', label = 'Prediction', markersize=3, alpha=.6)\n",
    "plt.legend()\n",
    "plt.xlabel('Temperature max in Fahrenheit'); plt.ylabel('Daily Rides'); plt.title('Actual vs Predicted Values');"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
