{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import numpy as np #numpy warnings may be safely ignored, known issue.\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from math import * # Used for the haversine/distance calculations\n",
    "import datetime as dt \n",
    "\n",
    "# for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for model building\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# for optimizations\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an empty DefaultDict\n",
    "# fill dictionary {year: station data}\n",
    "# build hierarchal dataframe using dictionary\n",
    "\n",
    "# Example of data location\n",
    "# https://raw.githubusercontent.com/SethDKelly/NiceRideMN/master/Nice_Ride_data/2010/NiceRide_station_2010.csv\n",
    "\n",
    "nr_dict = defaultdict()\n",
    "for x in [2010 + x for x in range(8)] :\n",
    "    nr_dict[x] = pd.read_csv(\"https://raw.githubusercontent.com/SethDKelly/NiceRideMN/master/Nice_Ride_data/\" \\\n",
    "                             +str(x)+\"/NiceRide_station_\"+str(x)+\".csv\")\n",
    "nr_station = pd.concat(nr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the most robust year/data set from the total\n",
    "nr_station_2017 = nr_station.loc[(2017),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Terminal</th>\n",
       "      <th>Station</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Ndocks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>30199</td>\n",
       "      <td>Hidden Falls Park</td>\n",
       "      <td>44.907001</td>\n",
       "      <td>-93.191432</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>30201</td>\n",
       "      <td>Elwood Ave N &amp; Oak Park Ave N</td>\n",
       "      <td>44.987160</td>\n",
       "      <td>-93.301944</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>30202</td>\n",
       "      <td>Mill City Quarter</td>\n",
       "      <td>44.980500</td>\n",
       "      <td>-93.261800</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>30203</td>\n",
       "      <td>W 50th Street &amp; W Minnehaha Pkwy</td>\n",
       "      <td>44.912125</td>\n",
       "      <td>-93.298552</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>30204</td>\n",
       "      <td>NE Central Ave &amp; NE 14th Ave</td>\n",
       "      <td>45.002526</td>\n",
       "      <td>-93.247162</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Terminal                           Station   Latitude  Longitude  Ndocks\n",
       "196     30199                 Hidden Falls Park  44.907001 -93.191432      19\n",
       "197     30201     Elwood Ave N & Oak Park Ave N  44.987160 -93.301944      15\n",
       "198     30202                 Mill City Quarter  44.980500 -93.261800      23\n",
       "199     30203  W 50th Street & W Minnehaha Pkwy  44.912125 -93.298552      23\n",
       "200     30204      NE Central Ave & NE 14th Ave  45.002526 -93.247162      15"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nr_station_2017.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First load in matrix that has the arc distance of each station comparatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = pd.read_csv(\"https://raw.githubusercontent.com/SethDKelly/NiceRideMN/master/Nice_Ride_data/2017/\"\\\n",
    "                        \"distance_matrix_2017.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "?pd.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 201 entries, 30000 to 30204\n",
      "Columns: 201 entries, 30000 to 30204\n",
      "dtypes: float64(201)\n",
      "memory usage: 317.2 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>30000</th>\n",
       "      <th>30001</th>\n",
       "      <th>30002</th>\n",
       "      <th>30003</th>\n",
       "      <th>30004</th>\n",
       "      <th>30005</th>\n",
       "      <th>30006</th>\n",
       "      <th>30007</th>\n",
       "      <th>30008</th>\n",
       "      <th>30009</th>\n",
       "      <th>...</th>\n",
       "      <th>30194</th>\n",
       "      <th>30195</th>\n",
       "      <th>30196</th>\n",
       "      <th>30197</th>\n",
       "      <th>30198</th>\n",
       "      <th>30199</th>\n",
       "      <th>30201</th>\n",
       "      <th>30202</th>\n",
       "      <th>30203</th>\n",
       "      <th>30204</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30000</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.500883</td>\n",
       "      <td>1.466835</td>\n",
       "      <td>2.471107</td>\n",
       "      <td>1.267481</td>\n",
       "      <td>0.304709</td>\n",
       "      <td>1.308283</td>\n",
       "      <td>1.034074</td>\n",
       "      <td>2.514716</td>\n",
       "      <td>0.970652</td>\n",
       "      <td>...</td>\n",
       "      <td>1.792963</td>\n",
       "      <td>1.091036</td>\n",
       "      <td>1.346930</td>\n",
       "      <td>1.120933</td>\n",
       "      <td>0.944879</td>\n",
       "      <td>6.253741</td>\n",
       "      <td>2.223943</td>\n",
       "      <td>0.397380</td>\n",
       "      <td>5.431379</td>\n",
       "      <td>1.301979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30001</th>\n",
       "      <td>2.500883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.062434</td>\n",
       "      <td>4.686271</td>\n",
       "      <td>2.976568</td>\n",
       "      <td>2.642299</td>\n",
       "      <td>2.790346</td>\n",
       "      <td>2.456370</td>\n",
       "      <td>1.899581</td>\n",
       "      <td>1.775162</td>\n",
       "      <td>...</td>\n",
       "      <td>1.178669</td>\n",
       "      <td>1.412001</td>\n",
       "      <td>1.362179</td>\n",
       "      <td>1.653395</td>\n",
       "      <td>1.579928</td>\n",
       "      <td>3.813231</td>\n",
       "      <td>4.358510</td>\n",
       "      <td>2.467683</td>\n",
       "      <td>4.825362</td>\n",
       "      <td>3.331634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30002</th>\n",
       "      <td>1.466835</td>\n",
       "      <td>1.062434</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.641812</td>\n",
       "      <td>1.967888</td>\n",
       "      <td>1.648298</td>\n",
       "      <td>1.808542</td>\n",
       "      <td>1.448112</td>\n",
       "      <td>1.612348</td>\n",
       "      <td>0.969751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977645</td>\n",
       "      <td>0.389264</td>\n",
       "      <td>0.738530</td>\n",
       "      <td>0.920689</td>\n",
       "      <td>0.633325</td>\n",
       "      <td>4.788731</td>\n",
       "      <td>3.322895</td>\n",
       "      <td>1.405513</td>\n",
       "      <td>4.753667</td>\n",
       "      <td>2.470703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30003</th>\n",
       "      <td>2.471107</td>\n",
       "      <td>4.686271</td>\n",
       "      <td>3.641812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.746448</td>\n",
       "      <td>2.564107</td>\n",
       "      <td>1.971116</td>\n",
       "      <td>2.246913</td>\n",
       "      <td>3.718459</td>\n",
       "      <td>3.438583</td>\n",
       "      <td>...</td>\n",
       "      <td>4.236012</td>\n",
       "      <td>3.364533</td>\n",
       "      <td>3.782869</td>\n",
       "      <td>3.585482</td>\n",
       "      <td>3.330279</td>\n",
       "      <td>8.096739</td>\n",
       "      <td>0.361928</td>\n",
       "      <td>2.300340</td>\n",
       "      <td>5.491521</td>\n",
       "      <td>2.988440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30004</th>\n",
       "      <td>1.267481</td>\n",
       "      <td>2.976568</td>\n",
       "      <td>1.967888</td>\n",
       "      <td>1.746448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.530430</td>\n",
       "      <td>0.242315</td>\n",
       "      <td>0.521591</td>\n",
       "      <td>2.057588</td>\n",
       "      <td>2.053101</td>\n",
       "      <td>...</td>\n",
       "      <td>2.722854</td>\n",
       "      <td>1.761634</td>\n",
       "      <td>2.284861</td>\n",
       "      <td>2.171021</td>\n",
       "      <td>1.813986</td>\n",
       "      <td>6.364891</td>\n",
       "      <td>1.399569</td>\n",
       "      <td>0.899256</td>\n",
       "      <td>4.474715</td>\n",
       "      <td>2.429523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          30000     30001     30002     30003     30004     30005     30006  \\\n",
       "30000  0.000000  2.500883  1.466835  2.471107  1.267481  0.304709  1.308283   \n",
       "30001  2.500883  0.000000  1.062434  4.686271  2.976568  2.642299  2.790346   \n",
       "30002  1.466835  1.062434  0.000000  3.641812  1.967888  1.648298  1.808542   \n",
       "30003  2.471107  4.686271  3.641812  0.000000  1.746448  2.564107  1.971116   \n",
       "30004  1.267481  2.976568  1.967888  1.746448  0.000000  1.530430  0.242315   \n",
       "\n",
       "          30007     30008     30009    ...        30194     30195     30196  \\\n",
       "30000  1.034074  2.514716  0.970652    ...     1.792963  1.091036  1.346930   \n",
       "30001  2.456370  1.899581  1.775162    ...     1.178669  1.412001  1.362179   \n",
       "30002  1.448112  1.612348  0.969751    ...     0.977645  0.389264  0.738530   \n",
       "30003  2.246913  3.718459  3.438583    ...     4.236012  3.364533  3.782869   \n",
       "30004  0.521591  2.057588  2.053101    ...     2.722854  1.761634  2.284861   \n",
       "\n",
       "          30197     30198     30199     30201     30202     30203     30204  \n",
       "30000  1.120933  0.944879  6.253741  2.223943  0.397380  5.431379  1.301979  \n",
       "30001  1.653395  1.579928  3.813231  4.358510  2.467683  4.825362  3.331634  \n",
       "30002  0.920689  0.633325  4.788731  3.322895  1.405513  4.753667  2.470703  \n",
       "30003  3.585482  3.330279  8.096739  0.361928  2.300340  5.491521  2.988440  \n",
       "30004  2.171021  1.813986  6.364891  1.399569  0.899256  4.474715  2.429523  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.info()\n",
    "distance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we wanted to merge our two new dataframes together\n",
    "# nr_station_2017 = pd.merge(nr_station_2017, distance, left_on='Terminal', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'distance_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-4cc89e5ca3d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdistance_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'distance_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS = []\n",
    "\n",
    "# k means determine k\n",
    "for k in range(2,10) :\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(distance_matrix)\n",
    "\n",
    "    SS.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(range(2,10), SS, 'bo-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('SS')\n",
    "plt.title('Elbow for Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "avg_sil = []\n",
    "\n",
    "#    Compute clustering algorithm (e.g., k-means clustering) for different values of k. For instance, by varying k from 1 to 10 clusters.\n",
    "for k in range(2,10) :\n",
    "    clusterer = KMeans(n_clusters=k, random_state=24).fit(distance_matrix)\n",
    "    cluster_labels = clusterer.labels_\n",
    "    \n",
    "#    For each k, calculate the average silhouette of observations (avg.sil).\n",
    "    silhouette_avg = silhouette_score(distance_matrix, cluster_labels)\n",
    "    print(\"For n_clusters =\", k, \"The average silhouette_score is :\", silhouette_avg)\n",
    "    avg_sil.append(silhouette_avg)\n",
    "    \n",
    "\n",
    "#    Plot the curve of avg.sil according to the number of clusters k.\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(range(2,10), avg_sil, 'bo-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Average Silhouette')\n",
    "plt.title('Elbow for Optimal k')\n",
    "plt.show()\n",
    "#    The location of the maximum is considered as the appropriate number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize a new PCA model with a default number of components.\n",
    "pca = PCA()\n",
    "pca.fit(distance_matrix)\n",
    "\n",
    "plt.figure(figsize=[14,8])\n",
    "plt.xlim(0,12)\n",
    "plt.plot(range(1,len(pca.explained_variance_)+1), pca.explained_variance_)\n",
    "plt.xticks(range(0,12,2))\n",
    "plt.ylabel('PCA Explained variance')\n",
    "plt.xlabel('Number of PCA dimensions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(PCA(n_components=2).fit_transform(distance_matrix), columns=['x','y'], index=nr_station_2017.index)\n",
    "pca_df = pd.DataFrame(nr_station_2017.Terminal).join(pca_df)\n",
    "    # Joins customer names to pca_df containing x-y PCA components\n",
    "print(pca_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ideal dimensionality is 2, and KMeans cluster 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_plot(pca_df, distance_matrix, x=4) :\n",
    "    pca_df['cluster']= KMeans(n_clusters=x).fit(distance_matrix).labels_\n",
    "    sns.lmplot(x='x', y='y', hue='cluster', data=pca_df, fit_reg=False, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_plot(pca_df, distance_matrix, 3)\n",
    "kmeans_plot(pca_df, distance_matrix, 4)\n",
    "kmeans_plot(pca_df, distance_matrix, 5)\n",
    "kmeans_plot(pca_df, distance_matrix, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the graphs: \n",
    "* n_clusters=3 possibly over simplified\n",
    "* n_clusters=4 possible insight\n",
    "* n_clusters=5 possible insight\n",
    "* n_clusters=6 possibly over-fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df['cluster']= KMeans(n_clusters=4).fit(distance_matrix).labels_\n",
    "four_nr_clusters = pd.merge(nr_station_2017, pca_df, left_on='Terminal', right_on='Terminal')\n",
    "pca_df['cluster']= KMeans(n_clusters=5).fit(distance_matrix).labels_\n",
    "five_nr_clusters = pd.merge(nr_station_2017, pca_df, left_on='Terminal', right_on='Terminal')\n",
    "pca_df['cluster']= KMeans(n_clusters=6).fit(distance_matrix).labels_\n",
    "six_nr_clusters = pd.merge(nr_station_2017, pca_df, left_on='Terminal', right_on='Terminal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations based on coordinates and clusters\n",
    "sns.lmplot(x='Longitude', y='Latitude', hue='cluster', data=four_nr_clusters, fit_reg=False, size=7)\n",
    "ax = plt.gca()\n",
    "ax.set_title('Four Clusters')\n",
    "sns.lmplot(x='Longitude', y='Latitude', hue='cluster', data=five_nr_clusters, fit_reg=False, size=7)\n",
    "ax = plt.gca()\n",
    "ax.set_title('Five Clusters')\n",
    "sns.lmplot(x='Longitude', y='Latitude', hue='cluster', data=six_nr_clusters, fit_reg=False, size=7)\n",
    "ax = plt.gca()\n",
    "ax.set_title('Six Clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_nr_clusters.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# requires dataframe with columns Latitude, Longitude, cluster\n",
    "\n",
    "def lat_long_sqkm(df, cluster=0)\n",
    "'''\n",
    "Returns the latitude longitude rectanglular area based on:\n",
    "Most northern-southern Latitude and eastern-western Longitude points\n",
    "based on designated cluster\n",
    "'''\n",
    "    lat_max = df[df.cluster == cluster].Latitude.max()\n",
    "    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1 \n",
    "    dlon = lon2 - lon1\n",
    "    area = dlat * dlon\n",
    "\n",
    "six_nr_clusters[six_nr_clusters.cluster == cluster].Latitude.max()\n",
    "six_nr_clusters[six_nr_clusters.cluster == 5].Latitude.min()\n",
    "six_nr_clusters[six_nr_clusters.cluster == 5].Longitude.max()\n",
    "six_nr_clusters[six_nr_clusters.cluster == 5].Longitude.min()\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    '''\n",
    "    Haversine definition: will calculate the distance between two points\n",
    "    using the latitude and longitude of each point.\n",
    "    '''\n",
    "    miles_constant = 3959\n",
    "    lat1, lon1, lat2, lon2 = map(np.deg2rad, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1 \n",
    "    dlon = lon2 - lon1 \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a)) \n",
    "    mi = miles_constant * c\n",
    "    return mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_2017 = pd.read_csv(\"https://raw.githubusercontent.com/SethDKelly/NiceRideMN/master/Nice_Ride_data/2017/NiceRide_trip_history_2017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_df = pd.DataFrame(rides_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_df.info()\n",
    "rides_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep/cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the start and end date columns to datetime\n",
    "\n",
    "rides_df['Start_date'] = rides_df['Start_date'].apply(lambda x: dt.datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))\n",
    "rides_df['End_date'] = rides_df['End_date'].apply(lambda x: dt.datetime.strptime(x,'%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting based on (non-)member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "member = rides_df[rides_df['account'] == \"Member\"]\n",
    "casual = rides_df[rides_df['account'] != \"Member\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_member = member[['Start_id', 'duration']]\n",
    "station_member = station_member.groupby(['Start_id']).count()\n",
    "station_member = station_member.rename(columns={'duration':'m_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casual_member = casual[['Start_id', 'duration']]\n",
    "casual_member = casual_member.groupby(['Start_id']).count()\n",
    "casual_member = casual_member.rename(columns={'duration':'c_count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the data to our clustered dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_nr_clusters = pd.merge(six_nr_clusters, station_member, left_on='Terminal', right_index=True)\n",
    "six_nr_clusters = pd.merge(six_nr_clusters, casual_member, left_on='Terminal', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = defaultdict()\n",
    "for x in np.unique(six_nr_clusters.cluster.values) :\n",
    "    temp['cluster_'+str(x)] = six_nr_clusters.groupby(six_nr_clusters.cluster==x)[['m_count', 'c_count']].describe()\n",
    "cluster_describe = pd.concat(temp)\n",
    "cluster_describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_nr_clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will find member mean rides per terminal per cluster\n",
    "cluster_summary = pd.DataFrame(columns=['terminals', 'tot_rides', 'm_rides', 'c_rides', 'mem_term', \\\n",
    "                                        'cas_term', 'pct_mem_rides'],\\\n",
    "                    index=['Cluster_'+str(x) for x in np.unique(six_nr_clusters.cluster.values)])\n",
    "\n",
    "for x in np.unique(six_nr_clusters.cluster.values) :\n",
    "    #build arrays by cluster to work with\n",
    "    member_array = six_nr_clusters[six_nr_clusters.cluster == x].m_count.values\n",
    "    casual_array = six_nr_clusters[six_nr_clusters.cluster == x].c_count.values\n",
    "    total_array = np.sum(member_array) + np.sum(casual_array)\n",
    "    \n",
    "    cluster_summary.loc['Cluster_'+str(x), 'terminals'] = len(member_array)\n",
    "    cluster_summary.loc['Cluster_'+str(x), 'tot_rides'] = member_array.sum() + casual_array.sum()\n",
    "    cluster_summary.loc['Cluster_'+str(x), 'm_rides'] = member_array.sum()\n",
    "    cluster_summary.loc['Cluster_'+str(x), 'c_rides'] = casual_array.sum()\n",
    "    cluster_summary.loc['Cluster_'+str(x), 'mem_term'] = member_array.sum()/len(member_array)\n",
    "    cluster_summary.loc['Cluster_'+str(x), 'cas_term'] = casual_array.sum()/len(casual_array)\n",
    "    cluster_summary.loc['Cluster_'+str(x), 'pct_mem_rides'] = (member_array.sum()/total_array.sum())*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster summaries looking at:\n",
    " * Total rides:\n",
    "     * Member\n",
    "     * Casual\n",
    " * Total terminals:\n",
    "     * Member mean rides per terminal\n",
    "     * Casual mean rides per terminal\n",
    " * Percent member riders of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_summary\n",
    "# possible to add arc-distance (Lat-Long) square miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations based on coordinates and clusters\n",
    "sns.lmplot(x='Longitude', y='Latitude', hue='cluster', data=six_nr_clusters, fit_reg=False, size=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intra/Trans-cluster travel\n",
    " * This can be split be (non-)membership too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a data frame with start terminal as indices, end terminal as columns filled with zeros\n",
    "travel_df = pd.DataFrame(columns = station_ids, index = station_ids).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = rides_df['Start_id'].values\n",
    "end = rides_df['End_id'].values\n",
    "\n",
    "for x in range(len(rides_df['Start_id'].values)) :\n",
    "    travel_df.loc[start[x], end[x]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "travel_matrix = travel_df.values\n",
    "normed_travel = normalize(travel_matrix, axis=1, norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clus_array = six_nr_clusters.cluster.values\n",
    "term_cluster_pair = {}\n",
    "for x in range(len(station_ids)) :\n",
    "    term_cluster_pair[station_ids[x]] = clus_array[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Build this better using lambda mapping and dictionaries\n",
    "'''\n",
    "travel_to = pd.DataFrame(columns=['to_0', 'to_1', 'to_2', 'to_3', 'to_4', 'to_5'], \\\n",
    "                              index=station_ids).fillna(0)\n",
    "\n",
    "for x in station_ids :\n",
    "    if term_cluster_pair[x] == 0:\n",
    "        travel_to['to_0'] += (travel_df[x].values)\n",
    "    elif term_cluster_pair[x] == 1:\n",
    "        travel_to['to_1'] += (travel_df[x].values)\n",
    "    elif term_cluster_pair[x] == 2:\n",
    "        travel_to['to_2'] += (travel_df[x].values)\n",
    "    elif term_cluster_pair[x] == 3:\n",
    "        travel_to['to_3'] += (travel_df[x].values)\n",
    "    elif term_cluster_pair[x] == 4:\n",
    "        travel_to['to_4'] += (travel_df[x].values)\n",
    "    elif term_cluster_pair[x] == 5:\n",
    "        travel_to['to_5'] += (travel_df[x].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to transpose the travel_matrix. This will give us instead of a data frame with start terminal as indices, end terminal as columns filled with zeros (from-to), a data frame with end terminals as indices and start terminals as columns (to-from).\n",
    "\n",
    "The purpose of this is to show how the clusters interact with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_matrix = travel_matrix.transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_df = pd.DataFrame(from_matrix, columns=station_ids, index=station_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Build this better using lambda mapping and dictionaries\n",
    "'''\n",
    "\n",
    "travel_from = pd.DataFrame(columns=['from_0', 'from_1', 'from_2', 'from_3', 'from_4', 'from_5'], \\\n",
    "                              index=station_ids).fillna(0)\n",
    "\n",
    "for x in station_ids :\n",
    "    if term_cluster_pair[x] == 0:\n",
    "        travel_from['from_0'] += (from_df[x].values)\n",
    "    elif term_cluster_pair[x] == 1:\n",
    "        travel_from['from_1'] += (from_df[x].values)\n",
    "    elif term_cluster_pair[x] == 2:\n",
    "        travel_from['from_2'] += (from_df[x].values)\n",
    "    elif term_cluster_pair[x] == 3:\n",
    "        travel_from['from_3'] += (from_df[x].values)\n",
    "    elif term_cluster_pair[x] == 4:\n",
    "        travel_from['from_4'] += (from_df[x].values)\n",
    "    elif term_cluster_pair[x] == 5:\n",
    "        travel_from['from_5'] += (from_df[x].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a super dataframe containing all the extra features we've created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_nr_clusters = pd.merge(six_nr_clusters, travel_to, left_on='Terminal', right_index=True)\n",
    "six_nr_clusters = pd.merge(six_nr_clusters, travel_from, left_on='Terminal', right_index=True)\n",
    "six_nr_clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_to_summary = pd.DataFrame(columns=['clus0', 'clus1', 'clus2','clus3', 'clus4', 'clus5', 'pct_intra'],\\\n",
    "                    index=['Cluster_'+str(x) for x in np.unique(six_nr_clusters.cluster.values)]).fillna(0)\n",
    "\n",
    "for x in np.unique(six_nr_clusters.cluster.values) :\n",
    "    intra_to_summary.loc['Cluster_'+str(x)]['clus0'] = six_nr_clusters[six_nr_clusters.cluster == x]['to_0'].values.sum()\n",
    "    intra_to_summary.loc['Cluster_'+str(x)]['clus1'] = six_nr_clusters[six_nr_clusters.cluster == x]['to_1'].values.sum()\n",
    "    intra_to_summary.loc['Cluster_'+str(x)]['clus2'] = six_nr_clusters[six_nr_clusters.cluster == x]['to_2'].values.sum()\n",
    "    intra_to_summary.loc['Cluster_'+str(x)]['clus3'] = six_nr_clusters[six_nr_clusters.cluster == x]['to_3'].values.sum()\n",
    "    intra_to_summary.loc['Cluster_'+str(x)]['clus4'] = six_nr_clusters[six_nr_clusters.cluster == x]['to_4'].values.sum()\n",
    "    intra_to_summary.loc['Cluster_'+str(x)]['clus5'] = six_nr_clusters[six_nr_clusters.cluster == x]['to_5'].values.sum()\n",
    "    intra_to_summary.loc['Cluster_'+str(x)]['pct_intra'] = (intra_to_summary.loc['Cluster_'+str(x)]['clus'+str(x)]\\\n",
    "                                                               / intra_to_summary.loc['Cluster_'+str(x),:].values.sum()) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_from_summary = pd.DataFrame(columns=['clus0', 'clus1', 'clus2','clus3', 'clus4', 'clus5', 'pct_intra'],\\\n",
    "                    index=['Cluster_'+str(x) for x in np.unique(six_nr_clusters.cluster.values)]).fillna(0)\n",
    "\n",
    "for x in np.unique(six_nr_clusters.cluster.values) :\n",
    "    intra_from_summary.loc['Cluster_'+str(x)]['clus0'] = six_nr_clusters[six_nr_clusters.cluster == x]['from_0'].values.sum()\n",
    "    intra_from_summary.loc['Cluster_'+str(x)]['clus1'] = six_nr_clusters[six_nr_clusters.cluster == x]['from_1'].values.sum()\n",
    "    intra_from_summary.loc['Cluster_'+str(x)]['clus2'] = six_nr_clusters[six_nr_clusters.cluster == x]['from_2'].values.sum()\n",
    "    intra_from_summary.loc['Cluster_'+str(x)]['clus3'] = six_nr_clusters[six_nr_clusters.cluster == x]['from_3'].values.sum()\n",
    "    intra_from_summary.loc['Cluster_'+str(x)]['clus4'] = six_nr_clusters[six_nr_clusters.cluster == x]['from_4'].values.sum()\n",
    "    intra_from_summary.loc['Cluster_'+str(x)]['clus5'] = six_nr_clusters[six_nr_clusters.cluster == x]['from_5'].values.sum()\n",
    "    intra_from_summary.loc['Cluster_'+str(x)]['pct_intra'] = (intra_from_summary.loc['Cluster_'+str(x)]['clus'+str(x)]\\\n",
    "                                                               / intra_from_summary.loc['Cluster_'+str(x),:].values.sum()) *100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Migration Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('To-From')\n",
    "print(intra_to_summary,'\\n')\n",
    "print('From-To')\n",
    "print(intra_from_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations based on coordinates and clusters\n",
    "sns.lmplot(x='Longitude', y='Latitude', hue='cluster', data=six_nr_clusters, fit_reg=False, size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for later optimization\n",
    "test = pd.DataFrame(intra_to_summary.values.T, \\\n",
    "             index=intra_to_summary.columns,\\\n",
    "            columns=['clus0', 'clus1', 'clus2','clus3', 'clus4', 'clus5']).fillna(0)\n",
    "\n",
    "#test['pct_intra'] = test.append([(test.loc['clus'+str(x)]['clus'+str(x)]\\\n",
    "#                                / test.loc['clus'+str(x),:].values.sum()) *100\\\n",
    "#                                for x in range(6)])\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix by station-station travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "normed_travel = normalize(travel_matrix, axis=1, norm='l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SS = []\n",
    "\n",
    "# k means determine k\n",
    "for k in range(2,30) :\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(normed_travel)\n",
    "\n",
    "    SS.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(range(2,30), SS, 'bo-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('SS')\n",
    "plt.title('Elbow for Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sil = []\n",
    "\n",
    "#    Compute clustering algorithm (e.g., k-means clustering) for different values of k. For instance, by varying k from 1 to 10 clusters.\n",
    "for k in range(2,200) :\n",
    "    clusterer = KMeans(n_clusters=k, random_state=24).fit(normed_travel)\n",
    "    cluster_labels = clusterer.labels_\n",
    "    \n",
    "#    For each k, calculate the average silhouette of observations (avg.sil).\n",
    "    avg_sil.append(silhouette_score(normed_travel, cluster_labels))\n",
    "    \n",
    "\n",
    "#    Plot the curve of avg.sil according to the number of clusters k.\n",
    "plt.figure(figsize=(14,8))\n",
    "plt.plot(range(2,200), avg_sil, 'bo-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Average Silhouette')\n",
    "plt.title('Elbow for Optimal k')\n",
    "plt.show()\n",
    "#    The location of the maximum is considered as the appropriate number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a new PCA model with a default number of components.\n",
    "pca = PCA()\n",
    "pca.fit(normed_travel)\n",
    "\n",
    "plt.figure(figsize=[14,8])\n",
    "plt.xlim(0,12)\n",
    "plt.plot(range(1,len(pca.explained_variance_)+1), pca.explained_variance_)\n",
    "plt.xticks(range(0,12,2))\n",
    "plt.ylabel('PCA Explained variance')\n",
    "plt.xlabel('Number of PCA dimensions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also assume 6 & 25 dimensions for final clustering, 2 will be used for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(PCA(n_components=2).fit_transform(normed_travel), columns=['x','y'], index=nr_station_2017.index)\n",
    "pca_df = pd.DataFrame(nr_station_2017.Terminal).join(pca_df)\n",
    "    # Joins terminal ID to pca_df containing x-y PCA components\n",
    "print(pca_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-From cluster visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pca_df['cluster']= KMeans(n_clusters=12).fit(normed_travel).labels_\n",
    "travel_cluster = pd.merge(nr_station_2017, pca_df, left_on='Terminal', right_on='Terminal')\n",
    "# Stations based on coordinates and clusters\n",
    "\n",
    "palette = sns.color_palette(\"Set1\", n_colors=12, desat=.7)\n",
    "sns.lmplot(x='Longitude', y='Latitude', hue='cluster', data=travel_cluster, fit_reg=False, palette=palette, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using transposition we'll see if there is any difference in a to-from clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_normed = normed_travel.transpose(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(PCA(n_components=2).fit_transform(from_normed), columns=['x','y'], index=nr_station_2017.index)\n",
    "pca_df = pd.DataFrame(nr_station_2017.Terminal).join(pca_df)\n",
    "    # Joins terminal ID to pca_df containing x-y PCA components\n",
    "print(pca_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From-To cluster visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df['cluster']= KMeans(n_clusters=24).fit(from_normed).labels_\n",
    "travel_cluster = pd.merge(nr_station_2017, pca_df, left_on='Terminal', right_on='Terminal')\n",
    "# Stations based on coordinates and clusters\n",
    "\n",
    "palette = sns.color_palette(\"Set1\", n_colors=12, desat=.7)\n",
    "sns.lmplot(x='Longitude', y='Latitude', hue='cluster', data=travel_cluster, fit_reg=False, palette=palette, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build cluster centers\n",
    "pca = pd.DataFrame(PCA(n_components=2).fit_transform(from_normed), columns=['x','y'], index=nr_station_2017.index)\n",
    "cluster_centers = KMeans(n_clusters=12).fit(pca).cluster_centers_ # Retrieves cluster center\n",
    "cluster_centers = pd.DataFrame(cluster_centers, columns=['x', 'y'])\n",
    "cluster_centers['cluster'] = range(0, len(cluster_centers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
